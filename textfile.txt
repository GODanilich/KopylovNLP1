Типы задач обработки естественного языка; распространённые варианты предобработки текста; трудности в их реализации.
Формальные аналитические грамматики; вероятностные модели; СММ; алгоритм Витерби.
Перцептрон; линейная ячейка; уравнение линейной ячейки; функция активации; порядок обучения нейронной сети.
Векторное представление слов; алгебраические операции над словами; модель Word2vec.
Рекуррентная сеть; модель Элмана; уравнения модели Элмана для последовательностей и для вектора скрытого состояния; недостатки рекуррентных сетей.
Модель Seq2seq; кодер и декодер; задачи для Seq2seq; долгая краткосрочная память.
Идея механизма внимания; внимание Богданова и Луонга; внутреннее внимание; multi-head attention.
Трансформер; позиционное кодирование; преимущества Трансформера; взаимосвязь кодера и декодера Трансформера.
BERT, GPT и прикладные вопросы использования генеративных моделей.
Идея генерации изображений и речи; диффузионная модель; MFCC; Tacotron2.